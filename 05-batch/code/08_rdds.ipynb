{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1927db0-7447-40ff-86a9-ae3ae8189823",
   "metadata": {},
   "source": [
    "# RDD = resilient distributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66f42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/01/26 08:59:08 WARN Utils: Your hostname, codespaces-f3c854 resolves to a loopback address: 127.0.0.1; using 10.0.0.133 instead (on interface eth0)\n",
      "26/01/26 08:59:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/01/26 08:59:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646fc343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cccd5",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fe52cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995d929e-f13c-4619-a69c-09ec7ca7476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd of the Spark df\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c244d9c-abdc-4016-b7d9-c1b7a29b488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_jrdd': JavaObject id=o43,\n",
       " 'is_cached': False,\n",
       " 'is_checkpointed': False,\n",
       " 'has_resource_profile': False,\n",
       " 'ctx': <SparkContext master=local[*] appName=test>,\n",
       " '_jrdd_deserializer': BatchedSerializer(CloudPickleSerializer(), -1),\n",
       " '_id': 13,\n",
       " 'partitioner': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21db6c-b40e-4ed6-af46-b29e0856052c",
   "metadata": {},
   "source": [
    "# First 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43a8f79-3e58-458a-bb2e-76d6fd64ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1880\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   1882\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m items += res\n\u001b[32m   1886\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   1484\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m sock_info = \u001b[38;5;28mself\u001b[39m._jvm.PythonRDD.runJob(\u001b[38;5;28mself\u001b[39m._jsc.sc(), \u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa3b98-d8b7-4740-ae35-44fbecbdc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibility issue between Python and PySpark versions, particularly when using Python 3.11 or newer with Spark 3.3.2.\n",
    "# This error happens during the serialization process when Spark tries to convert your DataFrame to an RDD and then execute the take(10) operation. \n",
    "# The root cause is often related to how PySpark's cloudpickle library handles object serialization in certain Python versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acef466d-ace5-474e-9803-faeb395e878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+\n",
      "|lpep_pickup_datetime|PULocationID|total_amount|\n",
      "+--------------------+------------+------------+\n",
      "| 2020-01-23 13:10:15|          74|       44.97|\n",
      "| 2020-01-20 15:09:00|          67|       33.45|\n",
      "| 2020-01-15 20:23:41|         260|         8.3|\n",
      "| 2020-01-05 16:32:26|          82|         8.3|\n",
      "| 2020-01-29 19:22:42|         166|       12.74|\n",
      "| 2020-01-15 11:07:42|         179|         5.8|\n",
      "| 2020-01-16 08:22:29|          41|       25.05|\n",
      "| 2020-01-28 17:05:28|          75|       21.33|\n",
      "| 2020-01-22 14:51:37|         152|         7.8|\n",
      "| 2020-01-31 10:25:04|          75|       26.05|\n",
      "+--------------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# approach 1 - show first 10 rows\n",
    "df_green.select('lpep_pickup_datetime', 'PULocationID', 'total_amount').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a3fea44-88f8-453b-9bc8-abc414077791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, total_amount=12.74),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 11, 7, 42), PULocationID=179, total_amount=5.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 16, 8, 22, 29), PULocationID=41, total_amount=25.05),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 17, 5, 28), PULocationID=75, total_amount=21.33),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 14, 51, 37), PULocationID=152, total_amount=7.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 10, 25, 4), PULocationID=75, total_amount=26.05)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a list -> no need .show()\n",
    "# approach 2 - return a list of the first 10 Row objects\n",
    "rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69dd326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# original\n",
    "# rows = rdd.take(10)\n",
    "\n",
    "# workaround\n",
    "rows = rdd.collect()[:10]\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4b7006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e827ac-242e-44a4-9917-1a9b438b3f69",
   "metadata": {},
   "source": [
    "# Filter on datetime field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dee2826-bc22-4817-9cad-f670ce5538e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ef2094b9-4eb4-4807-bc9d-ac9d361efa6c.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=test>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext  # This gets the SparkContext from SparkSession\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0bf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa2b00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round1\n",
    "# start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "# def filter_outliers(row):\n",
    "#     return row.lpep_pickup_datetime >= start\n",
    "\n",
    "\n",
    "# round2\n",
    "# import pickle\n",
    "# # Define the start date inside the function to avoid closure issues\n",
    "# def filter_outliers(row):\n",
    "#     start = datetime(year=2020, month=1, day=1)\n",
    "#     # Handle different row formats - try both attribute and index access\n",
    "#     try:\n",
    "#         pickup_time = row.lpep_pickup_datetime\n",
    "#     except AttributeError:\n",
    "#         try:\n",
    "#             # If row is a Row object, access by field name\n",
    "#             pickup_time = row['lpep_pickup_datetime']\n",
    "#         except (TypeError, KeyError):\n",
    "#             # If row is a tuple, access by index\n",
    "#             pickup_time = row[0]  # Adjust index based on your data structure\n",
    "    \n",
    "#     return pickup_time >= start\n",
    "\n",
    "# round 3\n",
    "from datetime import datetime\n",
    "from pyspark import Broadcast\n",
    "\n",
    "# Create broadcast variable for the start date\n",
    "start_date = sc.broadcast(datetime(year=2020, month=1, day=1))\n",
    "\n",
    "def filter_outliers(row):\n",
    "    # Access the broadcast variable\n",
    "    start = start_date.value\n",
    "    \n",
    "    # Handle different row types safely\n",
    "    if hasattr(row, 'lpep_pickup_datetime'):\n",
    "        pickup_time = row.lpep_pickup_datetime\n",
    "    elif isinstance(row, tuple) and len(row) >= 1:\n",
    "        # If it's a tuple, assume first element is lpep_pickup_datetime\n",
    "        pickup_time = row[0]\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    return pickup_time >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eac232c7-d132-45c0-9e01-15fef6d05b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not serialize object: IndexError: tuple index out of range\n",
      "Sample data structure:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_15619/3918720115.py\", line 3, in <module>\n",
      "    result = rdd.filter(filter_outliers).first()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1903, in first\n",
      "    rs = self.take(1)\n",
      "         ^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1883, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py\", line 1486, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "                                                           ^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 3505, in _jrdd\n",
      "    wrapped_func = _wrap_function(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 3362, in _wrap_function\n",
      "    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n",
      "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 3345, in _prepare_for_python_RDD\n",
      "    pickled_command = ser.dumps(command)\n",
      "                      ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 468, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: IndexError: tuple index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     result = \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_outliers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1903\u001b[39m, in \u001b[36mRDD.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1891\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1892\u001b[39m \u001b[33;03mReturn the first element in this RDD.\u001b[39;00m\n\u001b[32m   1893\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1901\u001b[39m \u001b[33;03mValueError: RDD is empty\u001b[39;00m\n\u001b[32m   1902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1903\u001b[39m rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1882\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m items += res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m sock_info = \u001b[38;5;28mself\u001b[39m._jvm.PythonRDD.runJob(\u001b[38;5;28mself\u001b[39m._jsc.sc(), \u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3344\u001b[39m ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Alternative: Try to see the structure of your data first\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample data structure:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m sample = \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(sample))\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1880\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   1882\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m items += res\n\u001b[32m   1886\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   1484\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m sock_info = \u001b[38;5;28mself\u001b[39m._jvm.PythonRDD.runJob(\u001b[38;5;28mself\u001b[39m._jsc.sc(), \u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# round 2\n",
    "# Try to get the first element\n",
    "try:\n",
    "    result = rdd.filter(filter_outliers).first()\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "    # Alternative: Try to see the structure of your data first\n",
    "    print(\"Sample data structure:\")\n",
    "    sample = rdd.take(1)[0]\n",
    "    print(type(sample))\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c7f48c-297e-4daa-8c5f-173c77a3384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# round3\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_outliers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1903\u001b[39m, in \u001b[36mRDD.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[T]\u001b[39m\u001b[33m\"\u001b[39m) -> T:\n\u001b[32m   1891\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1892\u001b[39m \u001b[33;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[32m   1893\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1901\u001b[39m \u001b[33;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[32m   1902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1903\u001b[39m     rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1904\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[32m   1905\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1880\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   1882\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m items += res\n\u001b[32m   1886\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   1484\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m sock_info = \u001b[38;5;28mself\u001b[39m._jvm.PythonRDD.runJob(\u001b[38;5;28mself\u001b[39m._jsc.sc(), \u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# round3\n",
    "result = rdd.filter(filter_outliers).first()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a8158b4-ef69-4225-b56b-d0dd3761851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pickup_time >= start\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result = \u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_outliers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1903\u001b[39m, in \u001b[36mRDD.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[T]\u001b[39m\u001b[33m\"\u001b[39m) -> T:\n\u001b[32m   1891\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1892\u001b[39m \u001b[33;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[32m   1893\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1901\u001b[39m \u001b[33;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[32m   1902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1903\u001b[39m     rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1904\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[32m   1905\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1880\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   1882\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m1883\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1885\u001b[39m items += res\n\u001b[32m   1886\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   1484\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m sock_info = \u001b[38;5;28mself\u001b[39m._jvm.PythonRDD.runJob(\u001b[38;5;28mself\u001b[39m._jsc.sc(), \u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# round 4\n",
    "from datetime import datetime\n",
    "\n",
    "def filter_outliers(row):\n",
    "    # Define start date inside the function to avoid closure capture\n",
    "    start = datetime(year=2020, month=1, day=1)\n",
    "    \n",
    "    # Handle different row structures\n",
    "    if hasattr(row, 'lpep_pickup_datetime'):\n",
    "        pickup_time = row.lpep_pickup_datetime\n",
    "    elif hasattr(row, '__getitem__') and len(row) > 0:\n",
    "        # Try to access by index if it's a tuple-like structure\n",
    "        try:\n",
    "            pickup_time = row[0]  # lpep_pickup_datetime is first column\n",
    "        except (TypeError, IndexError):\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    return pickup_time >= start\n",
    "\n",
    "result = rdd.filter(filter_outliers).first()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad70f6-11a3-45ad-9048-6774d5fbbcc8",
   "metadata": {},
   "source": [
    "# Group\n",
    "* map - Create k-v pair of (k=(hour, zone), v=(amount, count)) for `each row`\n",
    "* reduceByKey - combine amount and count `per (hour, zone) key`\n",
    "* map - unwrap / unpack into (hour, zone, total_amt, total_count) per `aggregated row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99eb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb328a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dae6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0a98ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a09200b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c14d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56ea72ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m df_result = \u001b[43mrdd\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_outliers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_for_grouping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_revenue\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m      5\u001b[39m     .map(unwrap) \\\n\u001b[32m      6\u001b[39m     .toDF(result_schema) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:2275\u001b[39m, in \u001b[36mRDD.reduceByKey\u001b[39m\u001b[34m(self, func, numPartitions, partitionFunc)\u001b[39m\n\u001b[32m   2252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduceByKey\u001b[39m(\n\u001b[32m   2253\u001b[39m     \u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[Tuple[K, V]]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2254\u001b[39m     func: Callable[[V, V], V],\n\u001b[32m   2255\u001b[39m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2256\u001b[39m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] = portable_hash,\n\u001b[32m   2257\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mRDD[Tuple[K, V]]\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2258\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2259\u001b[39m \u001b[33;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[32m   2260\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2273\u001b[39m \u001b[33;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[32m   2274\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:2558\u001b[39m, in \u001b[36mRDD.combineByKey\u001b[39m\u001b[34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[39m\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m merger.items()\n\u001b[32m   2557\u001b[39m locally_combined = \u001b[38;5;28mself\u001b[39m.mapPartitions(combineLocally, preservesPartitioning=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2558\u001b[39m shuffled = \u001b[43mlocally_combined\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mergeCombiners\u001b[39m(iterator: Iterable[Tuple[K, U]]) -> Iterable[Tuple[K, U]]:\n\u001b[32m   2561\u001b[39m     merger = ExternalMerger(agg, memory, serializer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:2486\u001b[39m, in \u001b[36mRDD.partitionBy\u001b[39m\u001b[34m(self, numPartitions, partitionFunc)\u001b[39m\n\u001b[32m   2483\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2485\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m-> \u001b[39m\u001b[32m2486\u001b[39m     pairRDD = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PairwiseRDD(\u001b[43mkeyed\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m.rdd()).asJavaPairRDD()\n\u001b[32m   2487\u001b[39m     jpartitioner = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonPartitioner(numPartitions, \u001b[38;5;28mid\u001b[39m(partitionFunc))\n\u001b[32m   2488\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3505\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3503\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[32m   3507\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3510\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier\n\u001b[32m   3512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3362\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   3360\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3361\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m3362\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3363\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m   3365\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m   3366\u001b[39m     env,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3371\u001b[39m     sc._javaAccumulator,\n\u001b[32m   3372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4675bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdfeee-5487-44e3-a1b3-63e6cc34a1d8",
   "metadata": {},
   "source": [
    "# mapPartitions - Gd for large datasets\n",
    "* input 1 partition\n",
    "* apply map fn\n",
    "* output 1 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "255b5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "645c3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "921e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f50db3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b8ecc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_distance']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6766c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7437b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # 2026.01 fix\n",
    "    # Spark does not recognise the Pandas Timestamp, converting it to string will solve the empty issues\n",
    "    df['lpep_pickup_datetime'] = df['lpep_pickup_datetime'].astype(str) # string conversion\n",
    "    \n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "580b5845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6055d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|             12.95|\n",
      "|             31.25|\n",
      "|              14.0|\n",
      "|             12.75|\n",
      "|               0.1|\n",
      "|             11.05|\n",
      "|11.299999999999999|\n",
      "|54.349999999999994|\n",
      "|             15.25|\n",
      "|             91.75|\n",
      "|             12.25|\n",
      "|               3.1|\n",
      "|               7.5|\n",
      "|11.899999999999999|\n",
      "| 78.89999999999999|\n",
      "|              4.45|\n",
      "|              23.2|\n",
      "|              4.85|\n",
      "|              6.65|\n",
      "|              15.1|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91d243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
