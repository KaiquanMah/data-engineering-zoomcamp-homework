{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72505747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd55afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/data-engineering-zoomcamp-homework/05-batch/spark/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f1cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6d80ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/01/25 08:03:00 WARN Utils: Your hostname, codespaces-870bd8 resolves to a loopback address: 127.0.0.1; using 10.0.4.172 instead (on interface eth0)\n",
      "26/01/25 08:03:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/01/25 08:03:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# how we connect to Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\    # local master - coordinate Spark jobs for ur cluster. [*] = use all available CPUs\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52682d8f-d216-43f4-b110-31eed0c044d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://62936c73-0076-449f-bab9-d0dec13356b2.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x778850db3080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7628576-51cc-4ebd-b796-2e16b1f64367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured executor cores: None\n"
     ]
    }
   ],
   "source": [
    "# Method 1 number of cores from Spark config\n",
    "cores_config = spark.sparkContext.getConf().get(\"spark.executor.cores\")\n",
    "print(f\"Configured executor cores: {cores_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae581eb-2b8a-4540-9ad4-8f66422c5d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available cores to Spark: 2\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Total cores Spark is using\n",
    "total_cores = spark.sparkContext.defaultParallelism\n",
    "print(f\"Total available cores to Spark: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a003e372-9357-4509-8890-174dc525b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local machine cores: 2\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Total cores from local machine\n",
    "import os\n",
    "driver_cores = os.cpu_count()\n",
    "print(f\"Local machine cores: {driver_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f604529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-25 08:07:49--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.169.24, 16.15.176.236, 16.15.177.162, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.169.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2026-01-25 08:07:50 ERROR 403: Forbidden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c88e722d-cad9-4182-955c-b5c2153bf324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-25 08:08:20--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 13.35.33.83, 13.35.33.60, 13.35.33.10, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|13.35.33.83|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-25 08:08:20 (1.30 GB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12342345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n"
     ]
    }
   ],
   "source": [
    "!head taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "809464d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e36dd996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb547351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2026.01.25 dont need to add .parquet file extension behind\n",
    "#   the .parquet file will be written to the 'zones' folder\n",
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02fe2bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 168K\n",
      "-rw-rw-rw-  1 codespace codespace  16K Jan 25 08:11 03_test.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace  19K Jan 24 08:56 04_pyspark.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace  33K Jan 24 08:56 05_taxi_schema.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace 8.7K Jan 24 08:56 06_spark_sql.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace 2.5K Jan 24 08:56 06_spark_sql.py\n",
      "-rw-rw-rw-  1 codespace codespace 2.6K Jan 24 08:56 06_spark_sql_big_query.py\n",
      "-rw-rw-rw-  1 codespace codespace 7.6K Jan 24 08:56 07_groupby_join.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace  12K Jan 24 08:56 08_rdds.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace 4.3K Jan 24 08:56 09_spark_gcs.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace 3.8K Jan 24 08:56 cloud.md\n",
      "-rw-rw-rw-  1 codespace codespace  544 Jan 24 08:56 download_data.sh\n",
      "-rw-rw-rw-  1 codespace codespace  17K Jan 24 08:56 homework.ipynb\n",
      "-rw-rw-rw-  1 codespace codespace  13K Feb 22  2024 taxi_zone_lookup.csv\n",
      "drwxr-xr-x+ 2 codespace codespace 4.0K Jan 25 08:11 zones\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f0812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
